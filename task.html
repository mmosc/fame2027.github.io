<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>POLYSIM 2026 - Task</title>
    <link rel="stylesheet" href="./static/css/styles.css">
</head>
<body>

    <!-- Navbar -->
    <nav>
        <div>
            <a href="index.html">Home</a>
            <a href="task.html">Task</a>
            <a href="resources.html">Resources</a>
            <a href="participate.html">Participate</a>
            <a href="references.html">References</a>
        </div>
    </nav>

    <div class="body-container">
        <!-- banner -->
        <div class="banner-container">
            <div class="card-container">
                <p>
                    <span style="font-family: montserrat_bold;"> OVERVIEW </span>
                </p>
            </div>
        </div>

        <!-- Task -->
        <div class="descriptions" id="task" style="padding-bottom: 4vh;">
            <p class="description-headings">Task</p>

            <!-- <p><b>Motivation</b></p> -->
            <p>
                Multimodal speaker classification systems typically assume the availability of all modalities
                during inference. However, in real-world scenarios such as multimedia retrieval, surveillance,
                and teleconferencing, one or more modalities may be unavailable due to occlusion, sensor
                failure, or data corruption. Additionally, most existing systems are trained and tested on the same language, limiting
                their applicability in multilingual environments.
            </p>
            <p>
                This challenge aims to push the boundaries of:
            </p>
            <ul>
                <li>Missing modality learning</li>
                <li>Multimodal robustness</li>
                <li>Cross-lingual speaker classification</li>
            </ul>

            <!-- <p class="description-headings">Task description and settings</p> -->
            <!-- <p><b></b></p> -->
            <p>
                The goal of the POLYSIM 2026 challenge is to identify speakers across multiple languages when either
                the face or voice modality is missing. Participants must develop models that can handle three scenarios:
                (1) voice-only identification when facial data is unavailable, (2) face-only identification when vocal
                data is unavailable, and (3) multimodal identification when both modalities are available. The challenge
                focuses on maintaining robust speaker identification performance across different languages (polyglot)
                even when one modality is completely absent, reflecting real-world constraints where complete multimodal
                data is not always available.
            </p>
            <p>
                The task is closed-set speaker classification using:
            </p>
            <ul>
                <li>Audio (voice)</li>
                <li>Visual (face)</li>
            </ul>
            <p>
                Participants must design a single unified model that can handle different testing conditions without retraining.
            </p>
            <div class="figures">
                <img src="./static/images/polysim_2026.jpg">
                <p>Figure 1: The POLYSIM 2026 Challenge focuses on speaker identification across multiple languages with
missing modalities (either face or voice may be absent).</p>
            </div>

            <!-- <p><b>Task Settings</b></p> -->
            <p>
                The challenge includes four task settings, covering multimodal, missing-modality, and cross-lingual scenarios.
            </p>

            <p><b>P3. In-Language Multimodal</b></p>
            <ul>
                <li><b>Training:</b> Audio + Face modalities</li>
                <li><b>Testing:</b> Audio + Face modalities</li>
                <li><b>Language:</b> Same language in training and testing</li>
            </ul>
            <p>
                This is the standard multimodal setting where both modalities are fully available and no language shift is present.
            </p>

            <p><b>P4. Missing-Modality (Audio-Only)</b></p>
            <ul>
                <li><b>Training:</b> Audio + Face modalities</li>
                <li><b>Testing:</b> Audio modality only</li>
                <li><b>Language:</b> Same language in training and testing</li>
            </ul>
            <p>
                The face modality is completely missing at test time. Models must perform speaker classification using audio only, without retraining.
            </p>

            <p><b>P5. Cross-Lingual Multimodal</b></p>
            <ul>
                <li><b>Training:</b> Audio + Face modalities</li>
                <li><b>Testing:</b> Audio + Face modalities</li>
                <li><b>Language:</b> Different languages for training and testing</li>
            </ul>
            <p>
                This setting evaluates the ability of models to generalize across languages when both modalities are available.
            </p>

            <p><b>P6. Cross-Lingual Missing-Modality</b></p>
            <ul>
                <li><b>Training:</b> Audio + Face modalities</li>
                <li><b>Testing:</b> Audio modality only</li>
                <li><b>Language:</b> Different languages for training and testing</li>
            </ul>
            <p>
                This is the most challenging scenario, combining:
            </p>
            <ul>
                <li>Cross-lingual testing</li>
                <li>Missing face modality during inference</li>
            </ul>

            <!-- <p><b>Task Settings Summary Table</b></p> -->
            <table style="width: 100%; border-collapse: collapse; margin-top: 20px; margin-bottom: 20px;">
                <thead>
                    <tr style="background-color: var(--main-color); color: var(--secondary-color);">
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Setting</th>
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Training Modalities</th>
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Testing Modalities</th>
                        <th style="border: 1px solid #ddd; padding: 12px; text-align: left;">Language</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 12px;"><b>P3</b></td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio + Face</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio + Face</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Same</td>
                    </tr>
                    <tr style="background-color: #f5f5f5;">
                        <td style="border: 1px solid #ddd; padding: 12px;"><b>P4</b></td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio + Face</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio only</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Same</td>
                    </tr>
                    <tr>
                        <td style="border: 1px solid #ddd; padding: 12px;"><b>P5</b></td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio + Face</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio + Face</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Cross-lingual</td>
                    </tr>
                    <tr style="background-color: #f5f5f5;">
                        <td style="border: 1px solid #ddd; padding: 12px;"><b>P6</b></td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio + Face</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Audio only</td>
                        <td style="border: 1px solid #ddd; padding: 12px;">Cross-lingual</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- Evaluation Protocol -->
        <div class="descriptions" id="evaluation" style="background-color: var(--tertiary-color); padding-bottom: 4vh;">
            <p class="description-headings">Evaluation protocol</p>

            <!-- <p><b>Metrics</b></p> -->
            <p>Performance will be evaluated using:</p>
            <ul>
                <li>Top-1 Accuracy</li>
                <li>Top-5 Accuracy</li>
                <li>Macro F1-Score</li>
            </ul>

            <p>
                We are also considering <b>Equal Error Rate (EER)</b> as an additional metric for
                evaluating the challenge performance. We expect the challenge participants to submit an output score
                file for every test pair to indicate how confident the system believes there is a match between the
                face and voice, or in other words, that the face and voice belong to the same person. The higher the
                score, the larger the confidence that the face and voice are from the same person. In real-world
                applications, people may set a threshold to determine if the pair belongs to same or different
                person as binary output. With a higher threshold, the false acceptance rate (FAR) will become lower,
                and the false rejection rate (FRR) will become higher. The EER is the optimal point when both errors
                FAR and FRR are equal. Therefore, EER is suitable to evaluate the performance of systems compared to
                conventional accuracy since it is independent of the threshold. Finally, a lower EER characterizes
                a better system.
                For more information please see the <a href="https://arxiv.org/abs/2508.04592">evaluation plan.</a>
            </p>

            <!-- <p><b>Ranking Criteria</b></p> -->
            <ul>
                <li>Scores are computed separately for P3, P4, P5, and P6</li>
                <li>Final ranking is based on the average performance across all settings</li>
                <li>Special emphasis is placed on P4 and P6 (missing-modality robustness)</li>
            </ul>
        </div>

        <!-- Footer -->
        <div class="footer">

        </div>
    </div>

</body>
</html>
