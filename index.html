<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>POLYSIM 2027 - Polyglot Speaker Identification with Missing Modality</title>
    <style>
        /* Including Monsterrat font */
        @font-face {
            font-family: 'montserrat_regular';
            src: url('./static/fonts/Montserrat/static/Montserrat-Regular.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-Regular.ttf') format('truetype'); */
        }

        @font-face {
            font-family: 'montserrat_medium';
            src: url('./static/fonts/Montserrat/static/Montserrat-Medium.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-Medium.ttf') format('truetype'); */
        }

        @font-face {
            font-family: 'montserrat_bold';
            src: url('./static/fonts/Montserrat/static/Montserrat-Bold.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-Bold.ttf') format('truetype'); */
        }

        @font-face {
            font-family: 'montserrat_lightItalic';
            src: url('./static/fonts/Montserrat/static/Montserrat-LightItalic.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-LightItalic.ttf') format('truetype'); */
        }

        /* //////////////////////////////////////////////////////////////////////// */


        /* Defining base properties */
        :root {
            /* --main-color: rgba(35, 35, 37, 1); */
            --main-color: #0d0c1d;
            --secondary-color: rgba(249, 249, 249, 1);
            --tertiary-color: rgba(230, 230, 230, 1);
            /* --tertiary-color: #d5e3f1; */
            /* --secondary-color: rgba(35, 35, 37, 1);
            --main-color: rgba(249, 249, 249, 1); */
        }

        html{
            scroll-behavior: smooth;
        }

        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            font-family: montserrat_regular;
            color: black;
            /* background-image: url("static/images/bg_image.jpg"); */
            background-image: linear-gradient(rgba(0, 0, 0, 0.4), rgba(0, 0, 0, 0.4)), url("static/images/bg_image.jpg");
            background-attachment: fixed;
            background-position: center;
            font-size: 16px;
            font-family: montserrat_regular;
            text-align: justify;
        }
        /* //////////////////////////////////////////////////////////////////////// */


        /* Navbar */
        nav {
            position: fixed;
            z-index: 100;
            background-color: var(--main-color);
            height: 10vh; /* 10% of the vertical height */
            width: 100%;
            display: flex;
            justify-content: center; /* Center the navigation */
            align-items: center;
            font-family: montserrat_medium;
            font-size: 18px;
        }

        nav div {
            display: flex;
            gap: 1.5vw;
            flex-wrap: wrap;
            justify-content: center;
            align-items: center;
        }

        nav a {
            text-decoration: none;
            color: var(--secondary-color);
            white-space: nowrap;
        }

        nav a:hover{
            color: #e74c3c; /* Change the color on hover */
        }
        /* //////////////////////////////////////////////////////////////////// */


        /* Body container */
        .body-container{
            width: 60%;
            background-color: var(--secondary-color);
            margin: 0;
        }
        /* /////////////////////////////////////////////////////////////////// */


        /* Banner */
        .banner-container{
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100%;
            height: 40vh;
            background-image: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url("./static/images/image_new.png");
            background-size: contain;
            background-position: center;
            background-repeat: no-repeat;
            background-position: top;
            margin-top: 10vh;
        }


        .card-container{
            display: flex;
            width: 350px;
            background-color: var(--secondary-color);
            opacity: 0.85; /* Set the desired opacity value */
            height: 100px;
            padding: 10px;
            justify-content: center;
            align-items: center;
            text-align: center;
            font-size: 26px;
            font-family: montserrat_medium;
            border-radius: 10px;
            box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.6); /* Set the box shadow */
            transition: transform 0.3s;

            /* background-color: #e74c3c; */
        }

        .card-container:hover{
            transform: scale(1.02);
        }
        /* ///////////////////////////////////////////////////////////////////// */


        /* Description data */
        .descriptions{
            padding: 0 40px;
        }

        .description-headings{
            font-family: montserrat_bold;
            font-size: 28px;
            padding-top: 8vh;
            padding-bottom: 2vh;
            text-align: center;
            margin: 0;
        }

        .figures{
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            font-family: montserrat_lightItalic;
            font-size: 12px;
            text-align: center;
        }

        .figures img{
            width: 80%;
        }

        .figures p{
            width: 80%;
        }
        /* ////////////////////////////////////////////////////////////////////// */

        /* Dataset */
        .dataset-container{
            display: flex;
            width: 100%;
            justify-content: space-between;
            margin-bottom: 20px;
        }

        .dataset-item{
            display: flex;
            flex-direction: column;
            width: 48%;
            text-align: left;
            height: 45vh;
        }

        /* Publications */
        .publications-container{
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }
        /* /////////////////////////////////////////////////////////////////// */


        /* Challenge button */
        .challenge-button-div{
            display: flex;
            justify-content: center;
            align-items: center;
        }
        
        .challenge-button{
            border-radius: 50px;
            /* height: 7vh; */
            padding: 10px;
            width: 40%;
            background-color: #e74c3c;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: transform 0.3s;
        }

        .challenge-button:hover{
            transform: scale(1.02);
            cursor: pointer;
        }
        /* /////////////////////////////////////////////////////////////////// */

        .footer{
            margin-top: 10vh;
            width: 100%;
            height: 5vh;
            background-color: var(--main-color);
        }
        
        @media (max-width: 1200px) {
            .body-container{
                width: 100%;
            }
        }

        @media (max-width: 900px) {
            nav {
                font-size: 14px;
                height: auto;
                min-height: 10vh;
                padding: 10px 0;
            }

            nav div {
                gap: 1vw;
            }

            .banner-container{
                height: 30vh;
            }

            .challenge-button{
                width: 80%;
            }

            .dataset-container{
                flex-direction: column;
            }

            .dataset-item{
                width: 100%;
            }
        }
    </style>
</head>
<body>

    <!-- Navbar -->
    <nav>
        <div>
            <a href="#home">Home</a>
            <a href="#task">Task</a>
            <a href="#dataset">Dataset</a>
            <a href="#baseline">Baseline</a>
            <a href="#evaluation">Evaluation</a>
            <a href="#submission">Submission</a>
            <a href="#registration">Registration</a>
            <a href="#organizers">Organizers</a>
        </div>
    </nav>
    <!-- //////////////////////////////////////////////////////////////////////// -->


    <div class="body-container" id="home">
        <!-- banner -->
        <div class="banner-container">
            <div class="card-container">
                <p>
                    <span style="font-family: montserrat_bold;"> MAV-CELEB </span><br> 
                    <span style="font-size: 14px;"> Multi-lingual Audio Visual dataset of Celebrities </span>
                </p>
                <p></p>
            </div>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Introduction -->
        <div class="descriptions" id="introduction">
            <p class="description-headings">INTRODUCTION</p>
            <p>
                Recent years have seen significant advances in multimodal biometric systems that leverage both facial and
                vocal characteristics for speaker identification. However, real-world applications often face scenarios where
                one modality is unavailable due to technical constraints, privacy concerns, or environmental factors. Building
                upon this challenge, we introduce the POLYSIM 2027 task that focuses on polyglot (multilingual) speaker
                identification when either the face or voice modality is missing. This addresses the critical question:
                "Can speakers be accurately identified across multiple languages when only partial multimodal information is available?".
                <br><br>
                This question is crucial for developing robust multilingual biometric systems that can operate under real-world
                constraints. To support this research, we provide the Multilingual Audio-Visual (MAV-CELEB) dataset, containing
                human speech clips of 154 identities with multiple language annotations extracted from various videos uploaded
                online. The dataset enables investigation of speaker identification across languages while handling missing
                modality scenarios, addressing both the polyglot and incomplete data challenges that are prevalent in
                practical applications.
            </p>
        </div>
        <div class="figures">
            <img src="./static/images/homepage_task_diag.jpg">
            <p>
                Figure 1: Diagram showing multimodal speaker identification across multiple languages with missing modality scenarios.
            </p>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Dataset -->
        <div class="descriptions" id="dataset">
            <p class="description-headings">DATASET</p>
            <p>
                The data is obtained from YouTube videos, consisting of celebrity interviews along with talk shows, 
                and television debates. The visual data spans over a vast range of variations including poses, 
                motion blur, background clutter, video quality, occlusions and lighting conditions. Moreover, most videos 
                contain real-world noise like background chatter, music, over-lapping speech, and compression artifacts, 
                resulting into a challenging dataset to evaluate multimedia systems.
            </p>
            
            <div class="dataset-container">
                <div class="dataset-item">
                    <p>The dataset is available on the following links:</p>
                    <ul>
                        <li>MAV-CELEB v1</li>
                        <ul>
                            <li><a href="https://drive.google.com/drive/folders/1OJyjXJULErvrvzLQmpJn5v8rRo0n_fod?usp=sharing">Raw files link</a></li>
                            <li><a href="https://drive.google.com/drive/folders/1MEHtEVh9lSa9hNZxjEfNJnE3qrpm_PKw?usp=sharing">Train/Test Split</a></li>
                        </ul>
                        <li>MAV-Celeb v3</li>
                        <ul>
                            <li><a href="">Raw files link</a></li>
                            <li><a href="">Train/Test Split</a></li>
                        </ul>
                    </ul>
                    <p>
                        To view the meta-data for the dataset, you can view the PDFs attached below:
                        <ul>
                            <li><a href="./static/docs/v1_meta.pdf">v1 meta-data file</a></li>
                            <li><a href="./static/docs/v3_meta.txt">v3 meta-data file</a></li>
                        </ul>
                    </p>
                </div>

                <div class="dataset-item" style="overflow-y: scroll;">
                    <p>The file structure is like:</p>
                    <img src="./static/images/dataset_structure_2.png">
                </div>
            </div>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Task -->
        <div class="descriptions" id="task">
            <p class="description-headings">TASK</p>
            <p><b>Polyglot Speaker Identification with Missing Modality</b></p>
            <p>
                The goal of the POLYSIM 2027 challenge is to identify speakers across multiple languages when either
                the face or voice modality is missing. Participants must develop models that can handle three scenarios:
                (1) voice-only identification when facial data is unavailable, (2) face-only identification when vocal
                data is unavailable, and (3) multimodal identification when both modalities are available. The challenge
                focuses on maintaining robust speaker identification performance across different languages (polyglot)
                even when one modality is completely absent, reflecting real-world constraints where complete multimodal
                data is not always available.
            </p>
            <div class="figures">
                <img src="./static/images/fame_2026.jpg">
                <p>Figure 2: The POLYSIM 2027 Challenge focuses on speaker identification across multiple languages with
missing modalities (either face or voice may be absent).</p>
            </div>
        </div>

        <!-- Baseline Model -->
        <div class="descriptions" id="baseline" style="background-color: var(--tertiary-color);">
            <p class="description-headings">BASELINE MODEL</p>
            <p>
                We provide a baseline model that has been trained on extracted features for facial and
                audio data (vggface for images and utterance level aggregator for voices). To learn a discriminative
                joint face-voice embedding for F-V association tasks, we develop a new framework for crossmodal face-voice
                association (See Fig. 3) that is fundamentally a two-stream pipeline and features a light-weight
                module that exploits complementary cues from both face and voice embeddings and facilitates discriminative identity
                mapping via orthogonality constraints
                <br> <br>
                Link to the paper:
                <a href="https://ieeexplore.ieee.org/abstract/document/9747704/">
                    Fusion and Orthogonal Projection for Improved Face-Voice Association
                </a>
                <br>
                Link to the Paper's code:
                <a href="https://github.com/msaadsaeed/FOP">
                    https://github.com/msaadsaeed/FOP
                </a>
                <br>
                Link to the Baseline code:
                <a href="https://github.com/mavceleb/mavceleb_baseline">
                    https://github.com/mavceleb/mavceleb_baseline
                </a>
            </p>
        </div>
        <div class="figures">
            <img src="./static/images/methodology.png">
            <p>Figure 3: Diagram showing our methodology.</p>
        </div>

        <!-- Evaluation Metrics -->
        <div class="descriptions" id="evaluation" style="background-color: var(--tertiary-color); padding-bottom: 2vh;">
            <p class="description-headings">EVALUATION METRICS</p>
            <p>
                We are considering <b>Equal Error Rate (EER)</b> as the metric for
                evaluating the challenge performance. We expect the challenge participants to submit a output score
                file for every test pairs to indicate how confident the system believes to have a match between the
                face and voice or in other words, the face and voice belongs to the same person. The higher the
                score is, the larger is the confidence of being the face and voice from the same person. In real-world
                applications, people may set a threshold to determine the if the pair belongs to same or different
                person as binary output. With the threshold higher, the false acceptance rate (FAR) will become lower,
                and the false rejection rate (FRR) will become higher. The EER is that optial point when both the errors
                FAR and FRR are equal. Therefore, EER becomes suitable to evaluate the performance of systems than the
                conventional accuracy since it independent of the threshold. Finally, the lower the EER it can characterize
                a better system.
                For more information please see <a href="https://arxiv.org/abs/2508.04592">evaluation plan.</a>
            </p>
        </div>

        <!-- Submission -->
        <div class="descriptions" id="submission">
            <p class="description-headings">SUBMISSION</p>
            <p>
                Within the directory containing the submission files, use
                zip archive.zip *.txt and do not zip the folder. Files should be
                named as:
                <ul>
                <code>
                    <li>sub score English heard.txt</li>
                    <li>sub score English unheard.txt</li>
                    <li>sub score Urdu heard.txt</li>
                    <li>sub score Urdu unheard.txt</li>
                </code>
                </ul>
                Files are submitted through Codalab in the evaluation phase
                3 times per day.
            </p>

            <p>
                We provide both train and test splits for v3 of MAV-Celeb dataset.
                Participants can use this split for fine-tuning their method. However, for v1 the test files
                are in format as below:
                <ul>
                <code>
                    <li>ysuvkz41 voices/English/00000.wav faces/English/00000.jpg</li>
                    <li>tog3zj45 voices/English/00001.wav faces/English/00001.jpg</li>
                    <li>ky5xfj1d voices/English/00002.wav faces/English/00002.jpg</li>
                    <li>yx4nfa35 voices/English/01062.wav faces/English/01062.jpg</li>
                    <li>bowsaf5e voices/English/01063.wav faces/English/01063.jpg</li>
                </code>
                </ul>
                We have kept the ground truth for fair evaluation during POLYSIM challenge. Participants are expected
                to compute and submit a text file including the id and L2 Scores in the following format:
                <ul>
                <code>
                    <li>ysuvkz41 0.9988</li>
                    <li>tog3zj45 0.1146</li>
                    <li>ky5xfj1d 0.6514</li>
                    <li>yx4nfa35 1.5321</li>
                    <li>bowsaf5e 1.6578</li>
                </code>
                </ul>
                The overall score will be computed as:<br>
                <p style="text-align: center;"><code><b>Overall Score = (Sum of all EERs) / 4</b></code></p>
                <br>
                Link to Codalab: <a href="https://www.codabench.org/competitions/9467/">Codalab/Codabench</a>
            </p>
        </div>

        <!-- Registration & Timeline -->
        <div class="descriptions" id="registration" style="background-color: var(--tertiary-color); padding-bottom: 2vh;">
            <p class="description-headings">REGISTRATION & TIMELINE</p>
            <p>Registration details for the "POLYSIM Challenge 2027" will be announced soon.</p>
            <p>For any queries please contact us at our email <a href="mailto:mavceleb@gmail.com">mavceleb@gmail.com</a>.</p>
            <br>
            <p><b>Timeline:</b></p>
            <ul>
                <li>Registration Period: 15 August – 15 October 2026</li>
                <li>Progress Phase: 1 September – 31 October 2026</li>
                <li>Evaluation Phase: 1 November – 15 November 2026</li>
                <li>Challenge Results: 16 November 2026</li>
                <li>Submission of System Descriptions: 25 November 2026</li>
                <li>ACM MM Grand Challenge Paper Submission: 07 December 2026</li>
            </ul>
        </div>

        <!-- Organizers -->
        <div class="descriptions" id="organizers">
            <p class="description-headings">ORGANIZERS</p>
            <p>
                <a href="https://hcai.at/persons/moscati/">Marta Moscati</a> -
                <span style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria</span> <br>

            <a href="https://ahmedembeddedxx.github.io/">Ahmed Abdullah</a> -
            <span style="font-size: 12px;">National University of Computer and Emerging Sciences, Pakistan</span> <br>

            <a href="https://scholar.google.com/citations?user=uyhEJ5IAAAAJ&hl=en">Muhammad Saad Saeed</a> -
            <span style="font-size: 12px;">University of Michigan, USA</span> <br>

            <a href="https://shahnawazgrewal.github.io/">Shah Nawaz</a> -
            <span style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria</span> <br>

            <a href="https://sites.google.com/view/rohankumardas">Rohan Kumar Das</a> -
            <span style="font-size: 12px;">Fortemedia Singapore, Singapore</span> <br>

            <a href="https://scholar.google.com/citations?user=nFxWrXEAAAAJ&hl=en">Muhammad Zaigham Zaheer</a> -
            <span style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates</span> <br>

            <a href="#">Junaid Mir</a> -
            <span style="font-size: 12px;">University of Engineering and Technology Taxila, Pakistan</span> <br>

            <a href="https://fms.uettaxila.edu.pk/Profile/haroon.yousaf">Muhammad Haroon Yousaf</a> -
            <span style="font-size: 12px;">University of Engineering and Technology Taxila, Pakistan</span> <br>

            <a href="https://scholar.google.com/citations?user=MZGQT2wAAAAJ&hl=en">Khalid Malik</a> -
            <span style="font-size: 12px;">University of Michigan, USA</span> <br>

            <a href="http://www.mschedl.eu/">Markus Schedl</a> -
            <span style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria | Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria</span> <br>

        </p>

        </div>

        <br>
        <br>
        <br>
        <br>
        <!-- Publications -->
        <div class="descriptions" id="publications" style="background-color: var(--tertiary-color);">
            <p class="description-headings">PUBLICATIONS</p>
            <div class="publications-container">
                
                <p>
                    <b>Dataset Paper</b>
                </p>
                <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Nawaz_Cross-Modal_Speaker_Verification_and_Recognition_A_Multilingual_Perspective_CVPRW_2021_paper.pdf"
                style="font-size: 14px;">
                    Cross-modal Speaker Verification and Recognition: A Multilingual Perspective
                </a>
                <p style="font-size: 14px; margin-bottom: 15px; width: 60%; text-align: center;">
                    <b>Authors:</b> Nawaz, Shah and Saeed, Muhammad Saad and Morerio, Pietro and Mahmood, Arif and Gallo, Ignazio and Yousaf, 
                    Muhammad Haroon and Del Bue, Alessio
                </p>

                <p>
                    <b>Baseline Paper</b>
                </p>
                <a href="https://ieeexplore.ieee.org/abstract/document/9747704/"
                style="font-size: 14px;">
                    Fusion and Orthogonal Projection for Improved Face-Voice Association
                </a>
                <p style="font-size: 14px; margin-bottom: 20px; width: 60%; text-align: center;">
                    <b>Authors:</b> Saeed, Muhammad Saad and Khan, Muhammad Haris and Nawaz, Shah and Yousaf, Muhammad Haroon and Del Bue, Alessio
                </p>

            </div>
        </div>
        <!-- /////////////////////////////////////////////////////////////////////// -->

        <!-- Past Challenges -->
        <div class="descriptions" id="past-challenges" style="background-color: var(--tertiary-color); padding-bottom: 4vh;">
            <p class="description-headings">PAST CHALLENGES</p>
            <div style="display: flex; flex-direction: column; align-items: center; gap: 20px;">
                <a href="mavceleb-2026/competition.html" style="all: initial; width: 100%; display: flex; justify-content: center;">
                    <div class="challenge-button-div" style="width: 100%;">
                        <div class="challenge-button" style="background: #888; opacity: 0.6; filter: grayscale(80%); box-shadow: none;">
                            <p class="description-headings" style="padding: 0; margin: 0%; color: #eee; text-decoration: line-through; font-style: italic;">
                            FAME Challenge 2026
                            </p>
                        </div>
                    </div>
                </a>

                <a href="mavceleb-2024/competition.html" style="all: initial; width: 100%; display: flex; justify-content: center;">
                    <div class="challenge-button-div" style="width: 100%;">
                        <div class="challenge-button" style="background: #888; opacity: 0.6; filter: grayscale(80%); box-shadow: none;">
                            <p class="description-headings" style="padding: 0; margin: 0%; color: #eee; text-decoration: line-through; font-style: italic;">
                            FAME Challenge 2024
                            </p>
                        </div>
                    </div>
                </a>
            </div>
        </div>

        <!-- ///////////////////////////////////////////////////////////////////////// -->
        
        <!-- Footer -->
        <div class="footer">

        </div>
    </div>
    

</body>
</html>
