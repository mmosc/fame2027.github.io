<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>POLYSIM 2026 - Resources</title>
    <link rel="stylesheet" href="./static/css/styles.css">
</head>
<body>

    <!-- Navbar -->
    <nav>
        <div>
            <a href="index.html">Home</a>
            <a href="task.html">Task</a>
            <a href="resources.html">Resources</a>
            <a href="participate.html">Participate</a>
            <a href="references.html">References</a>
        </div>
    </nav>

    <div class="body-container">
        <!-- banner -->
        <div class="banner-container">
            <div class="card-container">
                <p>
                    <span style="font-family: montserrat_bold;"> RESOURCES </span>
                </p>
            </div>
        </div>

        <!-- Dataset Description -->
        <div class="descriptions" id="dataset-description">
            <p class="description-headings">Dataset</p>

            <!-- <p><b>Dataset Overview</b></p> -->
            <p>
                The dataset consists of paired face images/video frames and speech audio samples collected from
                multiple speakers across different languages. The data is obtained from YouTube videos, consisting
                of celebrity interviews along with talk shows, and television debates. The visual data spans over
                a vast range of variations including poses, motion blur, background clutter, video quality, occlusions
                and lighting conditions. Moreover, most videos contain real-world noise like background chatter, music,
                over-lapping speech, and compression artifacts, resulting into a challenging dataset to evaluate
                multimedia systems.
            </p>

            <p><b>Modalities</b></p>
            <ul>
                <li><b>Audio:</b> Speech segments</li>
                <li><b>Visual:</b> Face images or face tracks</li>
                <li><b>Labels:</b> Speaker ID</li>
            </ul>

            <p><b>Data Splits</b></p>
            <ul>
                <li>Training set</li>
                <li>Validation set</li>
                <li>Test set (labels hidden)</li>
            </ul>

            <p><b>Missing Modality Setup</b></p>
            <ul>
                <li>Missing modalities occur only at test time</li>
                <li>Missing modality is explicit and complete (face modality absent)</li>
                <li>Training data always contains both modalities</li>
            </ul>
        </div>

        <!-- Dataset Access -->
        <div class="descriptions" id="dataset-access" style="background-color: var(--tertiary-color);">
            <p class="description-headings">Dataset access</p>

            <div class="dataset-container">
                <div class="dataset-item">
                    <p>The dataset is available on the following links:</p>
                    <ul>
                        <li>MAV-CELEB v1</li>
                        <ul>
                            <li><a href="https://drive.google.com/drive/folders/1OJyjXJULErvrvzLQmpJn5v8rRo0n_fod?usp=sharing">Raw files link</a></li>
                            <li><a href="https://drive.google.com/drive/folders/1MEHtEVh9lSa9hNZxjEfNJnE3qrpm_PKw?usp=sharing">Train/Test Split</a></li>
                        </ul>
                        <li>MAV-Celeb v3</li>
                        <ul>
                            <li><a href="">Raw files link</a></li>
                            <li><a href="">Train/Test Split</a></li>
                        </ul>
                    </ul>
                    <p>
                        To view the meta-data for the dataset, you can view the PDFs attached below:
                        <ul>
                            <li><a href="./static/docs/v1_meta.pdf">v1 meta-data file</a></li>
                            <li><a href="./static/docs/v3_meta.txt">v3 meta-data file</a></li>
                        </ul>
                    </p>
                </div>

                <div class="dataset-item" style="overflow-y: scroll;">
                    <p>The file structure is like:</p>
                    <img src="./static/images/dataset_structure_2.png">
                </div>
            </div>
        </div>

        <!-- Baseline Systems -->
        <div class="descriptions" id="baseline-systems">
            <p class="description-headings">Baseline systems</p>

            <p>
                We provide multiple baseline systems to help participants get started with the challenge.
                These baselines cover different modality configurations and fusion strategies:
            </p>

            <ul>
                <li><b>Audio-only baseline:</b> Speaker identification using only audio features</li>
                <li><b>Face-only baseline:</b> Speaker identification using only visual features</li>
                <li><b>Late-fusion multimodal baseline:</b> Combining predictions from both modalities</li>
                <li><b>Modality-dropout baseline:</b> Training with random modality dropout to handle missing modalities</li>
            </ul>

        <!-- Baseline Model -->
        <!-- <div class="descriptions" id="baseline-model" style="background-color: var(--tertiary-color);">
            <p class="description-headings">BASELINE MODEL</p>
            <p>
                We provide a baseline model that has been trained on extracted features for facial and
                audio data (vggface for images and utterance level aggregator for voices). To learn a discriminative
                joint face-voice embedding for F-V association tasks, we develop a new framework for crossmodal face-voice
                association (See Figure 1) that is fundamentally a two-stream pipeline and features a light-weight
                module that exploits complementary cues from both face and voice embeddings and facilitates discriminative identity
                mapping via orthogonality constraints.
            </p> -->
            <p>
                <b>Link to the paper:</b><br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9747704/">
                    Fusion and Orthogonal Projection for Improved Face-Voice Association
                </a>
            </p>
            <p>
                <b>Link to the Paper's code:</b><br>
                <a href="https://github.com/msaadsaeed/FOP">
                    https://github.com/msaadsaeed/FOP
                </a>
            </p>
            <p>
                <b>Link to the Baseline code:</b><br>
                <a href="https://github.com/mavceleb/mavceleb_baseline">
                    https://github.com/mavceleb/mavceleb_baseline
                </a>
            </p>
        </div>

        <div class="figures">
            <img src="./static/images/methodology.png">
            <p>Figure 1: Diagram showing our baseline methodology.</p>
        </div>

        <!-- Resources -->
        <!-- <div class="descriptions" id="resources" style="background-color: var(--tertiary-color); padding-bottom: 4vh;">
            <p class="description-headings">ADDITIONAL RESOURCES</p>

            <p><b>Dataset Download</b></p>
            <ul>
                <li><a href="https://drive.google.com/drive/folders/1OJyjXJULErvrvzLQmpJn5v8rRo0n_fod?usp=sharing">MAV-CELEB v1 Raw Files</a></li>
                <li><a href="https://drive.google.com/drive/folders/1MEHtEVh9lSa9hNZxjEfNJnE3qrpm_PKw?usp=sharing">MAV-CELEB v1 Train/Test Split</a></li>
            </ul>

            <p><b>Baseline Code Repository</b></p>
            <ul>
                <li><a href="https://github.com/mavceleb/mavceleb_baseline">MAV-CELEB Baseline Repository</a></li>
                <li><a href="https://github.com/msaadsaeed/FOP">FOP Method Repository</a></li>
            </ul>

            <p><b>Evaluation Scripts</b></p>
            <ul>
                <li>Evaluation scripts will be available in the baseline repository</li>
                <li>Script for computing Equal Error Rate (EER)</li>
                <li>Submission format validation scripts</li>
            </ul>

            <p><b>Documentation and FAQ</b></p>
            <ul>
                <li><a href="./static/docs/v1_meta.pdf">Dataset v1 Meta-data Documentation</a></li>
                <li><a href="./static/docs/v3_meta.txt">Dataset v3 Meta-data Documentation</a></li>
                <li>FAQ and troubleshooting guide (coming soon)</li>
            </ul>
        </div> -->

        <!-- Footer -->
        <div class="footer">

        </div>
    </div>

</body>
</html>
